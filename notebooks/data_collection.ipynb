{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a9ae19",
   "metadata": {},
   "source": [
    "## Heavy Lifter Collecting the Non-Hex Layers\n",
    "Needs to be cleaned up as it contains a good deal of cannibalized code and input from Claude Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e761d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection of functions needing to be initialized in order to be reused below. \n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Basic set up for grabbind data from ESRIs REST Server\n",
    "# Filtering using \"where\" is optional, defaults to getting everything with the 1=1\n",
    "# Need to update with bounding box/envelop input option\n",
    "def scrape_esri_rest(service_url: str, where: str = \"1=1\", out_fields: str = \"*\",\n",
    "                     batch_size: int = 1000, out_sr: int = 4326) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape all features from an Esri REST service endpoint.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    service_url : str\n",
    "        Full URL to the feature layer, e.g.\n",
    "        \"https://services.arcgis.com/.../FeatureServer/0\"\n",
    "    where : str\n",
    "        SQL where clause to filter features (default \"1=1\" = all).\n",
    "    out_fields : str\n",
    "        Comma-separated field names or \"*\" for all.\n",
    "    batch_size : int\n",
    "        Records per request (server may cap this, typically 1000-2000).\n",
    "    out_sr : int\n",
    "        Output spatial reference WKID (default 4326 = WGS84).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame with all attributes and geometry as a column.\n",
    "    \"\"\"\n",
    "    query_url = f\"{service_url.rstrip('/')}/query\"\n",
    "\n",
    "    # First, get the total count\n",
    "    count_params = {\n",
    "        \"where\": where,\n",
    "        \"returnCountOnly\": True,\n",
    "        \"f\": \"json\",\n",
    "    }\n",
    "    resp = requests.get(query_url, params=count_params)\n",
    "    resp.raise_for_status()\n",
    "    total = resp.json().get(\"count\", 0)\n",
    "    print(f\"Total features to fetch: {total}\")\n",
    "\n",
    "    # Paginate through all features\n",
    "    all_features = []\n",
    "    offset = 0\n",
    "\n",
    "    while offset < total:\n",
    "        params = {\n",
    "            \"where\": where,\n",
    "            \"outFields\": out_fields,\n",
    "            \"outSR\": out_sr,\n",
    "            \"f\": \"json\",\n",
    "            \"resultOffset\": offset,\n",
    "            \"resultRecordCount\": batch_size,\n",
    "            \"returnGeometry\": True,\n",
    "        }\n",
    "        resp = requests.get(query_url, params=params)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "\n",
    "        features = data.get(\"features\", [])\n",
    "        if not features:\n",
    "            break\n",
    "\n",
    "        for feat in features:\n",
    "            row = feat.get(\"attributes\", {})\n",
    "            geom = feat.get(\"geometry\")\n",
    "            if geom:\n",
    "                row[\"geometry\"] = json.dumps(geom)\n",
    "            all_features.append(row)\n",
    "\n",
    "        offset += len(features)\n",
    "        print(f\"  Fetched {offset}/{total}\")\n",
    "\n",
    "    df = pd.DataFrame(all_features)\n",
    "    print(f\"Done — {len(df)} records retrieved.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Geometry helpers\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, MultiPolygon, LinearRing\n",
    "from shapely.validation import make_valid\n",
    "\n",
    "\n",
    "def parse_geometry(g):\n",
    "    \"\"\"\n",
    "    Parse an Esri ring-based polygon geometry string into a Shapely geometry.\n",
    "\n",
    "    Handles both single-polygon and multi-part (MultiPolygon) cases by\n",
    "    inspecting ring winding order:\n",
    "      - Esri outer rings  → clockwise      (LinearRing.is_ccw == False)\n",
    "      - Esri hole rings   → counter-clockwise (LinearRing.is_ccw == True)\n",
    "\n",
    "    If there are multiple outer rings the result is a MultiPolygon.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = json.loads(g)\n",
    "        rings = parsed.get(\"rings\")\n",
    "        if not rings:\n",
    "            return None\n",
    "\n",
    "        outers, holes = [], []\n",
    "        for ring in rings:\n",
    "            lr = LinearRing(ring)\n",
    "            (holes if lr.is_ccw else outers).append(ring)\n",
    "\n",
    "        # Fallback: if winding detection gives no outers, treat first ring as outer\n",
    "        if not outers:\n",
    "            outers, holes = [rings[0]], rings[1:]\n",
    "\n",
    "        if len(outers) == 1:\n",
    "            geom = Polygon(outers[0], holes)\n",
    "        else:\n",
    "            # Multiple outer rings → MultiPolygon; assign holes to their parent\n",
    "            polys = []\n",
    "            for outer in outers:\n",
    "                outer_poly = Polygon(outer)\n",
    "                assigned = [h for h in holes if Polygon(h).within(outer_poly)]\n",
    "                polys.append(Polygon(outer, assigned))\n",
    "            geom = MultiPolygon(polys)\n",
    "\n",
    "        return make_valid(geom)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  parse_geometry error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Save the json\n",
    "def save_esri(s, out_dir, layer_name, format=\"gpkg\"):\n",
    "    # Convert Esri geometry to shapely and save as GeoPackage\n",
    "    df_temp = s[s[\"geometry\"].notna()].copy()\n",
    "    df_temp[\"_geom\"] = df_temp[\"geometry\"].apply(parse_geometry)\n",
    "    df_temp = df_temp[df_temp[\"_geom\"].notna()]\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df_temp.drop(columns=[\"geometry\", \"_geom\"]),\n",
    "        geometry=df_temp[\"_geom\"].tolist(),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    drivers = {\"gpkg\": \"GPKG\", \"geojson\": \"GeoJSON\"}\n",
    "    gdf.to_file(out_dir / f\"{layer_name}.{format}\", driver=drivers[format])\n",
    "    print(f\"Saved to {out_dir.resolve()} ({len(gdf)} features)\")\n",
    "\n",
    "# For use with direct download links that serve up zipped folders\n",
    "def download_zip(url: str, out_dir) -> Path:\n",
    "    \"\"\"\n",
    "    Download a zip archive from a URL and extract it to out_dir.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url     : str   Direct download URL for the .zip file.\n",
    "    out_dir : str | Path  Destination directory (created if needed).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path  The resolved output directory.\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    r = requests.get(url, stream=True)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content)) as zf:\n",
    "        zf.extractall(out_dir)\n",
    "        extracted = zf.namelist()\n",
    "\n",
    "    print(f\"Extracted {len(extracted)} files → {out_dir.resolve()}\")\n",
    "    return out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92bddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tippecanoe image already exists\n"
     ]
    }
   ],
   "source": [
    "# Prepping and running Tippecanoe with local Docker container\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def to_docker_path(p):\n",
    "    return str(Path(p).resolve()).replace(\"\\\\\", \"/\")\n",
    "\n",
    "base  = Path(\"..\").resolve()\n",
    "tiles = base / \"tiles\"\n",
    "tiles.mkdir(exist_ok=True)\n",
    "\n",
    "# Build local tippecanoe image if not already built\n",
    "image_check = subprocess.run([\"docker\", \"image\", \"inspect\", \"tippecanoe\"], capture_output=True)\n",
    "if image_check.returncode != 0:\n",
    "    print(\"Building tippecanoe Docker image (one-time, takes ~2 min)...\")\n",
    "    build = subprocess.run(\n",
    "        [\"docker\", \"build\", \"-t\", \"tippecanoe\",\n",
    "         \"-f\", f\"{to_docker_path(base)}/Dockerfile.tippecanoe\", str(base)],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    print(\"Image built successfully\" if build.returncode == 0 else f\"Build failed:\\n{build.stderr}\")\n",
    "else:\n",
    "    print(\"tippecanoe image already exists\")\n",
    "\n",
    "\n",
    "def run_tippecanoe(layers, base=base, tiles=tiles):\n",
    "    \"\"\"\n",
    "    Tile a list of GeoJSON layers using Tippecanoe via Docker.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layers : list of (layer_name, geojson_path_relative_to_base, min_zoom, max_zoom)\n",
    "    base   : Path  root directory mounted into Docker as /data\n",
    "    tiles  : Path  output directory for .mbtiles files\n",
    "    \"\"\"\n",
    "    for layer_name, geojson_rel, zmin, zmax in layers:\n",
    "        cmd = [\n",
    "            \"docker\", \"run\", \"--rm\",\n",
    "            \"-v\", f\"{to_docker_path(base)}:/data\",\n",
    "            \"tippecanoe\",\n",
    "            \"tippecanoe\",\n",
    "            \"-o\", f\"/data/tiles/{layer_name}.mbtiles\",\n",
    "            f\"-Z{zmin}\", f\"-z{zmax}\",\n",
    "            \"--layer\", layer_name,\n",
    "            \"--drop-densest-as-needed\",\n",
    "            \"--maximum-tile-bytes\", \"500000\",\n",
    "            \"--force\",\n",
    "            f\"/data/{geojson_rel}\"\n",
    "        ]\n",
    "        print(f\"Running Tippecanoe: {layer_name}...\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            size = (tiles / f\"{layer_name}.mbtiles\").stat().st_size / 1e6\n",
    "            print(f\"  Done — {size:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"  Error:\\n{result.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2789bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to ship off the tiles to Mapbox\n",
    "\n",
    "\n",
    "import boto3\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MAPBOX_USERNAME = \"keystonecarto\"\n",
    "st_upload_write = os.environ[\"MAPBOX_SECRET_WRITE\"]\n",
    "st_upload_read  = os.environ[\"MAPBOX_SECRET_READ\"]\n",
    "\n",
    "def upload_to_mapbox(mbtiles_path: Path, tileset_id: str, st_w, st_r):\n",
    "    \"\"\"\n",
    "    Upload an .mbtiles file to Mapbox as a tileset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mbtiles_path : Path   Local path to the .mbtiles file.\n",
    "    tileset_id   : str   Tileset name without username prefix, e.g. 'ny_dec_lands'.\n",
    "                         Final tileset will be '{username}.{tileset_id}'.\n",
    "    \"\"\"\n",
    "    tileset_full = f\"{MAPBOX_USERNAME}.{tileset_id}\"\n",
    "    print(f\"Uploading {mbtiles_path.name} → {tileset_full}\")\n",
    "\n",
    "    # Step 1 — get temporary S3 staging credentials\n",
    "    creds_resp = requests.get(\n",
    "        f\"https://api.mapbox.com/uploads/v1/{MAPBOX_USERNAME}/credentials\",\n",
    "        params={\"access_token\": st_w}\n",
    "    )\n",
    "    creds_resp.raise_for_status()\n",
    "    creds = creds_resp.json()\n",
    "\n",
    "    # Step 2 — upload file to Mapbox's S3 staging bucket\n",
    "    print(\"  Uploading to S3 staging...\")\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=creds[\"accessKeyId\"],\n",
    "        aws_secret_access_key=creds[\"secretAccessKey\"],\n",
    "        aws_session_token=creds[\"sessionToken\"],\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    "    s3.upload_file(str(mbtiles_path), creds[\"bucket\"], creds[\"key\"])\n",
    "    print(\"  S3 upload complete\")\n",
    "\n",
    "    # Step 3 — trigger Mapbox processing\n",
    "    upload_resp = requests.post(\n",
    "        f\"https://api.mapbox.com/uploads/v1/{MAPBOX_USERNAME}\",\n",
    "        params={\"access_token\": st_w},\n",
    "        json={\n",
    "            \"url\": f\"https://{creds['bucket']}.s3.amazonaws.com/{creds['key']}\",\n",
    "            \"tileset\": tileset_full,\n",
    "        }\n",
    "    )\n",
    "    upload_resp.raise_for_status()\n",
    "    upload_id = upload_resp.json()[\"id\"]\n",
    "    print(f\"  Processing started (upload id: {upload_id})\")\n",
    "\n",
    "    # Step 4 — poll until complete or failed\n",
    "    # Modify this with a time out, high risk of infinite loop and really it's unnecessary\n",
    "    while True:\n",
    "        status_resp = requests.get(\n",
    "            f\"https://api.mapbox.com/uploads/v1/{MAPBOX_USERNAME}/{upload_id}\",\n",
    "            params={\"access_token\": st_r}\n",
    "        )\n",
    "        status = status_resp.json()\n",
    "        if status.get(\"complete\"):\n",
    "            print(f\"  Done — tileset ready: mapbox://{tileset_full}\")\n",
    "            return tileset_full\n",
    "        if status.get(\"error\"):\n",
    "            raise RuntimeError(f\"Mapbox upload failed: {status['error']}\")\n",
    "        print(f\"  Processing... ({status.get('progress', 0)*100:.0f}%)\")\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14460ce0",
   "metadata": {},
   "source": [
    "## Start of the Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb33ae1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features to fetch: 90\n",
      "  Fetched 90/90\n",
      "Done — 90 records retrieved.\n",
      "Saved to C:\\data\\repos\\onx_hunt\\data\\hunting_units\\nj (90 features)\n",
      "Total features to fetch: 92\n",
      "  Fetched 92/92\n",
      "Done — 92 records retrieved.\n",
      "Saved to C:\\data\\repos\\onx_hunt\\data\\hunting_units\\ny (92 features)\n",
      "Downloading PGC_BNDWildlifeManagementUnits2021.zip...\n",
      "Extracted 9 files → C:\\data\\repos\\onx_hunt\\data\\hunting_units\\pa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('../data/hunting_units/pa')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hunting Units\n",
    "\n",
    "# New Jersey\n",
    "\n",
    "service_url = \"https://mapsdep.nj.gov/arcgis/rest/services/Features/Environmental_admin/MapServer/17\"\n",
    "out_dir = Path(\"../data/hunting_units/nj/\")\n",
    "layer_name = 'deer_management_zones'\n",
    "file_type = 'gpkg'\n",
    "\n",
    "df = scrape_esri_rest(service_url, batch_size=2000)\n",
    "\n",
    "# Convert geojson to geopackage and save\n",
    "save_esri(df, out_dir, layer_name, file_type)\n",
    "\n",
    "\n",
    "# New York\n",
    "\n",
    "service_url = \"https://services6.arcgis.com/DZHaqZm9cxOD4CWM/ArcGIS/rest/services/Wildlife_Management_Units/FeatureServer/0\"\n",
    "out_dir = Path(\"../data/hunting_units/ny/\")\n",
    "layer_name = 'wildlife_management_units'\n",
    "file_type = 'gpkg'\n",
    "\n",
    "df = scrape_esri_rest(service_url, batch_size=2000)\n",
    "\n",
    "# Convert geojson to geopackage and save\n",
    "save_esri(df, out_dir, layer_name, file_type)\n",
    "\n",
    "\n",
    "# Pennsylvania\n",
    "source_url = \"https://www.pasda.psu.edu/download/pgc/PGC_BNDWildlifeManagementUnits2021.zip\"\n",
    "dest_folder = \"../data/hunting_units/pa/\"\n",
    "\n",
    "download_zip(\n",
    "    source_url,\n",
    "    dest_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43d39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"USE_LABEL\" IN ('State Park', 'State Forest')\n",
      "Total features to fetch: 8726\n",
      "  Fetched 2000/8726\n",
      "  Fetched 4000/8726\n",
      "  Fetched 6000/8726\n",
      "  Fetched 8000/8726\n",
      "  Fetched 8726/8726\n",
      "Done — 8726 records retrieved.\n",
      "Saved to C:\\data\\repos\\onx_hunt\\data\\public_lands\\nj (8726 features)\n",
      "Total features to fetch: 3233\n",
      "  Fetched 2000/3233\n",
      "  Fetched 3233/3233\n",
      "Done — 3233 records retrieved.\n",
      "Saved to C:\\data\\repos\\onx_hunt\\data\\public_lands\\ny (3233 features)\n",
      "Downloading PGC_StateGameland202301.zip...\n",
      "Extracted 9 files → C:\\data\\repos\\onx_hunt\\data\\public_lands\\pa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('../data/public_lands/pa')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Public Lands\n",
    "\n",
    "# Scrape NJ\n",
    "# Going to go easy on their server and filter extra out since this is based off of parcels\n",
    "\n",
    "service_url = \"https://mapsdep.nj.gov/arcgis/rest/services/Features/Land/MapServer/65\"\n",
    "out_dir = Path(\"../data/public_lands/nj/\")\n",
    "layer_name = 'parks_and_forests'\n",
    "\n",
    "filt_col = 'USE_LABEL'\n",
    "filt_vals = ['State Park', 'State Forest']\n",
    "where = f'\"{filt_col}\" IN ({\", \".join(f\"\\'{v}\\'\" for v in filt_vals)})'\n",
    "\n",
    "print(where)\n",
    "\n",
    "df = scrape_esri_rest(service_url, batch_size=2000, where=where)\n",
    "\n",
    "\n",
    "# Convert geojson to geopackage and save\n",
    "save_esri(df, out_dir, layer_name)\n",
    "\n",
    "# Scrape NYS DEC Lands (state forests, wildlife mgmt areas, etc.)\n",
    "service_url = \"https://services6.arcgis.com/DZHaqZm9cxOD4CWM/ArcGIS/rest/services/NYS_DEC_Lands/FeatureServer/0\"\n",
    "out_dir = Path(\"../data/public_lands/ny/\")\n",
    "layer_name = 'dec_lands'\n",
    "file_type = 'gpkg'\n",
    "\n",
    "df = scrape_esri_rest(service_url, batch_size=2000)\n",
    "\n",
    "# Convert geojson to geopackage and save\n",
    "save_esri(df, out_dir, layer_name, file_type)\n",
    "\n",
    "\n",
    "# Pennsylvania\n",
    "\n",
    "source_url = \"https://www.pasda.psu.edu/download/pgc/PGC_StateGameland202301.zip\"\n",
    "dest_folder = \"../data/public_lands/pa/\"\n",
    "\n",
    "download_zip(\n",
    "    source_url,\n",
    "    dest_folder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded07c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features to fetch: 378\n",
      "  Fetched 378/378\n",
      "Done — 378 records retrieved.\n",
      "Saved to C:\\data\\repos\\onx_hunt\\data\\misc\\harvest (362 features)\n"
     ]
    }
   ],
   "source": [
    "# Harvest Data\n",
    "# Just PA for now, other states are all in PDFs\n",
    "\n",
    "service_url = 'https://services1.arcgis.com/k8yxvICm95iIFicb/ArcGIS/rest/services/Historic_Harvest_Data_for_Dashboards/FeatureServer/0'\n",
    "out_dir = Path(\"../data/misc/harvest\")\n",
    "layer_name = 'harvest_v1'\n",
    "\n",
    "df = scrape_esri_rest(service_url, batch_size=2000)\n",
    "\n",
    "# Convert geojson to geopackage and save\n",
    "save_esri(df, out_dir, layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6573cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 3 features saved\n",
      "Counties: 150 features saved\n",
      "Mask saved — 169.5 KB\n",
      "Running Tippecanoe: state_mask...\n",
      "  Done — 1.1 MB\n",
      "Uploading state_mask.mbtiles → keystonecarto.state_mask\n",
      "  Uploading to S3 staging...\n",
      "  S3 upload complete\n",
      "  Processing started (upload id: cmls48e540jx11mnqz4qxgv63)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Done — tileset ready: mapbox://keystonecarto.state_mask\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'keystonecarto.state_mask'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Census Data\n",
    "\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "state_fips = ['36', '34', '42']  # NY, NJ, PA\n",
    "out_dir = Path(\"../data/census\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# States — Census Cartographic Boundary (500k = simplified, good for display)\n",
    "states = gpd.read_file(\"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_us_state_500k.zip\")\n",
    "states = states[states['STATEFP'].isin(state_fips)]\n",
    "states.to_file(out_dir / \"states.geojson\", driver=\"GeoJSON\")\n",
    "print(f\"States: {len(states)} features saved\")\n",
    "\n",
    "# Counties\n",
    "counties = gpd.read_file(\"https://www2.census.gov/geo/tiger/GENZ2023/shp/cb_2023_us_county_500k.zip\")\n",
    "counties = counties[counties['STATEFP'].isin(state_fips)]\n",
    "counties.to_file(out_dir / \"counties.geojson\", driver=\"GeoJSON\")\n",
    "print(f\"Counties: {len(counties)} features saved\")\n",
    "\n",
    "\n",
    "# Create the Mask to highlight the region\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import unary_union\n",
    "from pathlib import Path\n",
    "\n",
    "states = gpd.read_file(\"../data/census/states.geojson\")\n",
    "\n",
    "# Large box — covers visible map area well beyond the 3 states\n",
    "world_box = box(-180, -85, 180, 85)\n",
    "\n",
    "# Union the 3 states into one shape, then punch them out of the box\n",
    "states_union = unary_union(states.geometry)\n",
    "mask_geom = world_box.difference(states_union)\n",
    "\n",
    "mask_gdf = gpd.GeoDataFrame({'geometry': [mask_geom]}, crs='EPSG:4326')\n",
    "out_dir = Path(\"../data/census\")\n",
    "mask_gdf.to_file(out_dir / \"state_mask.geojson\", driver=\"GeoJSON\")\n",
    "print(f\"Mask saved — {(out_dir / 'state_mask.geojson').stat().st_size / 1e3:.1f} KB\")\n",
    "\n",
    "run_tippecanoe([\n",
    "    (\"state_mask\", \"data/census/state_mask.geojson\", 2, 7),\n",
    "])\n",
    "\n",
    "upload_to_mapbox(tiles / \"state_mask.mbtiles\", \"state_mask\", st_upload_write, st_upload_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cdcdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Tippecanoe: census_states...\n",
      "  Done — 0.2 MB\n",
      "Running Tippecanoe: census_counties...\n",
      "  Done — 0.8 MB\n",
      "Uploading census_states.mbtiles → keystonecarto.census_states\n",
      "  Uploading to S3 staging...\n",
      "  S3 upload complete\n",
      "  Processing started (upload id: cmls4af6l1ksq1mqztd2nwjhk)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Done — tileset ready: mapbox://keystonecarto.census_states\n",
      "Uploading census_counties.mbtiles → keystonecarto.census_counties\n",
      "  Uploading to S3 staging...\n",
      "  S3 upload complete\n",
      "  Processing started (upload id: cmls4ay1308n41nlda9kfqsdn)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Done — tileset ready: mapbox://keystonecarto.census_counties\n"
     ]
    }
   ],
   "source": [
    "# Generating the tiles\n",
    "\n",
    "run_tippecanoe([\n",
    "    (\"census_states\",   \"data/census/states.geojson\",   2, 10),\n",
    "    (\"census_counties\", \"data/census/counties.geojson\",  4, 10),\n",
    "])\n",
    "\n",
    "\n",
    "for layer in [\"census_states\", \"census_counties\"]:\n",
    "    upload_to_mapbox(tiles / f\"{layer}.mbtiles\", layer, st_upload_write, st_upload_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80e3b76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features to fetch: 288\n",
      "  Fetched 288/288\n",
      "Done — 288 records retrieved.\n",
      "Saved to C:\\data\\repos\\onx_hunt\\data\\misc (0 features)\n"
     ]
    }
   ],
   "source": [
    "# Seasonal Roads: State forest roads that are normally closed to vehicle traffic, \n",
    "# which will be opened to hunters between the dates for the current year's hunting season.\n",
    "\n",
    "service_url = \"https://maps.dcnr.pa.gov/agsprod/rest/services/BOF/HuntStateForest/MapServer/9\"\n",
    "out_dir = Path(\"../data/misc/\")\n",
    "layer_name = 'seasonal_rds'\n",
    "file_type = 'gpkg'\n",
    "\n",
    "df = scrape_esri_rest(service_url, batch_size=2000)\n",
    "\n",
    "# Convert geojson to geopackage and save\n",
    "save_esri(df, out_dir, layer_name, file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f51b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features to fetch: 9\n",
      "  Fetched 9/9\n",
      "Done — 9 records retrieved.\n",
      "Saved to C:\\data\\repos\\onx_hunt\\data\\misc (9 features)\n"
     ]
    }
   ],
   "source": [
    "# Chronic Wasting Disease\n",
    "\n",
    "\n",
    "# # Could make a time series animation or slider out of the images\n",
    "\n",
    "# # National\n",
    "# source_url = 'https://www.sciencebase.gov/catalog/file/get/58068050e4b0824b2d1d415d?f=__disk__f8/ad/16/f8ad16fb7ebd525ff13433fd73d569e5da41d8b6'\n",
    "# dest_folder = \"../data/misc/national/\"\n",
    "\n",
    "# download_zip(\n",
    "#     source_url,\n",
    "#     dest_folder\n",
    "# )\n",
    "\n",
    "service_url = \"https://pgcmaps.pa.gov/arcgis/rest/services/PGC/DMAs/MapServer/0\"\n",
    "out_dir = Path(\"../data/misc/\")\n",
    "layer_name = 'cwd_disease_management_area'\n",
    "file_type = 'geojson'\n",
    "\n",
    "df = scrape_esri_rest(service_url, batch_size=2000)\n",
    "\n",
    "# Convert geojson to geopackage and save\n",
    "save_esri(df, out_dir, layer_name, file_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea07d55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063 POIs saved to C:\\data\\repos\\onx_hunt\\data\\poi\\hunting_stores.geojson\n",
      "1063 → 514 POIs after clipping to NY/NJ/PA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\culms\\AppData\\Local\\Temp\\ipykernel_17220\\3855057227.py:57: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: EPSG:4326\n",
      "Right CRS: EPSG:4269\n",
      "\n",
      "  poi_clipped = gpd.sjoin(poi, states[['geometry']], how='inner', predicate='within')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done — 0.8 MB\n",
      "Uploading hunting_stores.mbtiles → keystonecarto.hunting_stores\n",
      "  Uploading to S3 staging...\n",
      "  S3 upload complete\n",
      "  Processing started (upload id: cmls4f9k41kcl1nqzlys27rut)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Done — tileset ready: mapbox://keystonecarto.hunting_stores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'keystonecarto.hunting_stores'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get points of interest from OSM using the API\n",
    "\n",
    "import requests, json\n",
    "from pathlib import Path\n",
    "\n",
    "# Bounding box: NY/NJ/PA  (S, W, N, E)\n",
    "# Remove this hardcoded value and replace\n",
    "bbox = \"38.9,-80.5,45.1,-71.8\"\n",
    "\n",
    "query = f\"\"\"\n",
    "[out:json][timeout:60];\n",
    "(\n",
    "  node[\"shop\"=\"sports\"]({bbox});\n",
    "  node[\"shop\"=\"firearm\"]({bbox});\n",
    "  node[\"leisure\"=\"archery\"]({bbox});\n",
    "  node[\"sport\"=\"archery\"]({bbox});\n",
    ");\n",
    "out body;\n",
    "\"\"\"\n",
    "\n",
    "r = requests.post(\"https://overpass-api.de/api/interpreter\", data={\"data\": query})\n",
    "elements = r.json()[\"elements\"]\n",
    "\n",
    "features = []\n",
    "for el in elements:\n",
    "    tags = el.get(\"tags\", {})\n",
    "    features.append({\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\"type\": \"Point\", \"coordinates\": [el[\"lon\"], el[\"lat\"]]},\n",
    "        \"properties\": {\n",
    "            \"name\":    tags.get(\"name\", \"\"),\n",
    "            \"shop\":    tags.get(\"shop\", \"\"),\n",
    "            \"sport\":   tags.get(\"sport\", \"\"),\n",
    "            \"leisure\": tags.get(\"leisure\", \"\"),\n",
    "            \"phone\":   tags.get(\"phone\", \"\"),\n",
    "            \"website\": tags.get(\"website\", \"\"),\n",
    "            \"addr\":    tags.get(\"addr:street\", \"\"),\n",
    "        }\n",
    "    })\n",
    "\n",
    "out_dir = Path(\"../data/poi\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_path = out_dir / \"hunting_stores.geojson\"\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump({\"type\": \"FeatureCollection\", \"features\": features}, f)\n",
    "\n",
    "print(f\"{len(features)} POIs saved to {out_path.resolve()}\")\n",
    "\n",
    "# Clean up the excess that fall outside the detailed border\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "poi = gpd.read_file(\"../data/poi/hunting_stores.geojson\")\n",
    "states = gpd.read_file(\"../data/census/states.geojson\")\n",
    "\n",
    "# Spatial filter — keep only points that fall within the 3 states\n",
    "poi_clipped = gpd.sjoin(poi, states[['geometry']], how='inner', predicate='within')\n",
    "poi_clipped = poi_clipped.drop(columns=['index_right'])\n",
    "\n",
    "out_path = Path(\"../data/poi/hunting_stores.geojson\")\n",
    "poi_clipped.to_file(out_path, driver=\"GeoJSON\")\n",
    "print(f\"{len(poi)} → {len(poi_clipped)} POIs after clipping to NY/NJ/PA\")\n",
    "\n",
    "\n",
    "\n",
    "# Alternate Tippecanoe method\n",
    "# POI — no feature dropping, keep all points at all zooms\n",
    "# No real need for clustering in this iteration\n",
    "layer_name = \"hunting_stores\"\n",
    "geojson_rel = \"data/poi/hunting_stores.geojson\"\n",
    "\n",
    "cmd = [\n",
    "    \"docker\", \"run\", \"--rm\",\n",
    "    \"-v\", f\"{to_docker_path(base)}:/data\",\n",
    "    \"tippecanoe\", \"tippecanoe\",\n",
    "    \"-o\", f\"/data/tiles/{layer_name}.mbtiles\",\n",
    "    \"-Z2\", \"-z14\",\n",
    "    \"--layer\", layer_name,\n",
    "    \"-r1\",       # rate=1: keep every feature, no dropping\n",
    "    \"--force\",\n",
    "    f\"/data/{geojson_rel}\"\n",
    "]\n",
    "import subprocess\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    size = (tiles / f\"{layer_name}.mbtiles\").stat().st_size / 1e6\n",
    "    print(f\"Done — {size:.1f} MB\")\n",
    "else:\n",
    "    print(result.stderr)\n",
    "\n",
    "upload_to_mapbox(\n",
    "    tiles / \"hunting_stores.mbtiles\",\n",
    "    \"hunting_stores\",\n",
    "    st_upload_write,\n",
    "    st_upload_read\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6dc73",
   "metadata": {},
   "source": [
    "## Normalize the Regional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2abe4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 12268 features → ..\\data\\public_lands\\public_lands.geojson\n",
      "state\n",
      "NJ    8726\n",
      "NY    3233\n",
      "PA     309\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Combine NJ parks/forests, NY DEC lands, PA state game lands into public_lands.geojson\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import MultiPolygon\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path(\"../data/public_lands\")\n",
    "\n",
    "# --- NJ Parks and Forests ---\n",
    "nj = gpd.read_file(out_dir / \"nj/parks_and_forests.gpkg\")\n",
    "nj = nj[['NAME_LABEL', 'MANAGED_BY', 'USE_LABEL', 'GISACRES', 'geometry']].rename(columns={\n",
    "    'NAME_LABEL': 'name',\n",
    "    'MANAGED_BY': 'managed_by',\n",
    "    'USE_LABEL':  'land_type',\n",
    "    'GISACRES':   'acres',\n",
    "})\n",
    "nj['state'] = 'NJ'\n",
    "\n",
    "# --- NY DEC Lands ---\n",
    "ny = gpd.read_file(out_dir / \"ny/dec_lands.gpkg\")\n",
    "ny = ny[['FACILITY', 'CATEGORY', 'ACRES', 'geometry']].rename(columns={\n",
    "    'FACILITY': 'name',\n",
    "    'CATEGORY': 'land_type',\n",
    "    'ACRES': 'acres',\n",
    "})\n",
    "ny['state']      = 'NY'\n",
    "ny['managed_by'] = 'NY DEC'\n",
    "\n",
    "# --- PA State Game Lands ---\n",
    "pa = gpd.read_file(out_dir / \"pa/PGC_StateGameland202301.shp\").to_crs(epsg=4326)\n",
    "pa['name'] = 'State Game Land ' + pa['SGL'].astype(str).str.zfill(3)\n",
    "pa = pa[['name', 'ACRES', 'geometry']].rename(columns={'ACRES': 'acres'})\n",
    "pa['state'] = 'PA'\n",
    "pa['land_type'] = 'State Game Land'\n",
    "pa['managed_by'] = 'PA Game Commission'\n",
    "\n",
    "# --- Combine and normalize geometry to MultiPolygon ---\n",
    "combined = gpd.GeoDataFrame(\n",
    "    gpd.pd.concat([nj, ny, pa], ignore_index=True),\n",
    "    crs='EPSG:4326'\n",
    ")[['state', 'land_type', 'name', 'managed_by', 'acres', 'geometry']]\n",
    "\n",
    "def to_multi(geom):\n",
    "    if geom is None or geom.is_empty:\n",
    "        return None\n",
    "    return MultiPolygon([geom]) if geom.geom_type == 'Polygon' else geom\n",
    "\n",
    "combined['geometry'] = combined['geometry'].apply(to_multi)\n",
    "combined = combined[combined['geometry'].notna()].reset_index(drop=True)\n",
    "\n",
    "out_path = out_dir / \"public_lands.geojson\"\n",
    "combined.to_file(out_path, driver='GeoJSON')\n",
    "print(f\"Saved {len(combined)} features → {out_path}\")\n",
    "print(combined.groupby('state').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8427f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Tippecanoe: public_lands...\n",
      "  Done — 10.3 MB\n",
      "Uploading public_lands.mbtiles → keystonecarto.public_lands\n",
      "  Uploading to S3 staging...\n",
      "  S3 upload complete\n",
      "  Processing started (upload id: cmls4gwll1k2g1np5e0vovq88)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Done — tileset ready: mapbox://keystonecarto.public_lands\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'keystonecarto.public_lands'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tippecanoe([\n",
    "    (\"public_lands\", \"data/public_lands/public_lands.geojson\", 5, 13),\n",
    "])\n",
    "\n",
    "upload_to_mapbox(\n",
    "    tiles / \"public_lands.mbtiles\",\n",
    "    \"public_lands\",\n",
    "    st_upload_write,\n",
    "    st_upload_read\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd284d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMZ   : 90 features\n",
      "UNIT  : 92 features\n",
      "WMU_ID: 23 features\n",
      "\n",
      "Saved 205 total features → C:\\data\\repos\\onx_hunt\\data\\hunting_units\\hunting_units.geojson\n"
     ]
    }
   ],
   "source": [
    "# Merge the hunting units\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import MultiPolygon\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Config: (file, zone_type label, id column) ---\n",
    "sources = [\n",
    "    (\"../data/hunting_units/nj/deer_management_zones.gpkg\", \"DMZ\", \"DMZ\"),\n",
    "    (\"../data/hunting_units/ny/wildlife_management_units.gpkg\", \"UNIT\", \"UNIT\"),\n",
    "    (\"../data/hunting_units/pa/PGC_BNDWildlifeManagementUnits2021.shp\", \"WMU_ID\", \"WMU_ID\"),\n",
    "]\n",
    "\n",
    "# Generalize this function and ship up top\n",
    "def to_multipolygon(geom):\n",
    "    \"\"\"Normalize Polygon → MultiPolygon so all features share one geometry type.\"\"\"\n",
    "    if geom is None:\n",
    "        return None\n",
    "    if geom.geom_type == \"Polygon\":\n",
    "        return MultiPolygon([geom])\n",
    "    return geom\n",
    "\n",
    "layers = []\n",
    "for path, zone_type, id_col in sources:\n",
    "    gdf = gpd.read_file(path)\n",
    "\n",
    "    # Reproject to WGS84 if needed\n",
    "    if gdf.crs.to_epsg() != 4326:\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "    # Normalize geometry type to MultiPolygon\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].apply(to_multipolygon)\n",
    "\n",
    "    # Add zone_type + zone_id, drop everything else\n",
    "    gdf[\"zone_type\"] = zone_type\n",
    "    gdf[\"zone_id\"]   = gdf[id_col].astype(str)\n",
    "    gdf = gdf[[\"zone_type\", \"zone_id\", \"geometry\"]]\n",
    "\n",
    "    layers.append(gdf)\n",
    "    print(f\"{zone_type:6s}: {len(gdf)} features\")\n",
    "\n",
    "# --- Combine and save ---\n",
    "combined = gpd.GeoDataFrame(\n",
    "    gpd.pd.concat(layers, ignore_index=True),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# out_path = Path(\"../data/hunting_units/hunting_units.gpkg\")\n",
    "out_path = Path(\"../data/hunting_units/hunting_units.geojson\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "# combined.to_file(out_path, driver=\"GPKG\")\n",
    "combined.to_file(out_path, driver=\"GeoJSON\")\n",
    "print(f\"\\nSaved {len(combined)} total features → {out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54869348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Tippecanoe: hunting_units...\n",
      "  Done — 0.7 MB\n",
      "Uploading hunting_units.mbtiles → keystonecarto.hunting_units\n",
      "  Uploading to S3 staging...\n",
      "  S3 upload complete\n",
      "  Processing started (upload id: cmls4i79e1kqq1pqz2dcqsj5u)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Done — tileset ready: mapbox://keystonecarto.hunting_units\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'keystonecarto.hunting_units'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tippecanoe([\n",
    "    (\"hunting_units\",     \"/data/hunting_units/hunting_units.geojson\",           0, 9),\n",
    "])\n",
    "\n",
    "upload_to_mapbox(\n",
    "    tiles / \"hunting_units.mbtiles\",\n",
    "    \"hunting_units\",\n",
    "    st_upload_write,\n",
    "    st_upload_read\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f2e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Tippecanoe: att_res6...\n",
      "  Done — 2.2 MB\n",
      "Running Tippecanoe: att_res7...\n",
      "  Done — 4.3 MB\n",
      "Running Tippecanoe: att_res8...\n",
      "  Done — 20.1 MB\n",
      "Uploading att_res6.mbtiles → keystonecarto.att_res6\n",
      "  Uploading to S3 staging...\n",
      "  S3 upload complete\n",
      "  Processing started (upload id: cmls4n0v30ajb1qs52zufqg0y)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Done — tileset ready: mapbox://keystonecarto.att_res6\n",
      "Uploading att_res7.mbtiles → keystonecarto.att_res7\n",
      "  Uploading to S3 staging...\n",
      "  S3 upload complete\n",
      "  Processing started (upload id: cmls4nl6o13fo1oo6pk8ix89c)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Done — tileset ready: mapbox://keystonecarto.att_res7\n",
      "Uploading att_res8.mbtiles → keystonecarto.att_res8\n",
      "  Uploading to S3 staging...\n",
      "  S3 upload complete\n",
      "  Processing started (upload id: cmls4odzz08rv1pphkkhccacr)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Processing... (0%)\n",
      "  Done — tileset ready: mapbox://keystonecarto.att_res8\n"
     ]
    }
   ],
   "source": [
    "# Create the MNO/AT&T tiles here since the rest of the code is here\n",
    "\n",
    "run_tippecanoe([\n",
    "    (\"att_res6\",     \"scraped_data/mno/att/h3/att_res6.geojson\",           0, 7),\n",
    "    (\"att_res7\",     \"scraped_data/mno/att/h3/att_res7.geojson\",           7, 9),\n",
    "    (\"att_res8\",     \"scraped_data/mno/att/h3/att_res8.geojson\",           9, 11),\n",
    "])\n",
    "\n",
    "layers_to_upload = [\n",
    "    (\"att_res6\",     \"scraped_data/mno/att/h3/att_res6.geojson\",           0, 7),\n",
    "    (\"att_res7\",     \"scraped_data/mno/att/h3/att_res7.geojson\",           7, 9),\n",
    "    (\"att_res8\",     \"scraped_data/mno/att/h3/att_res8.geojson\",          9, 11),\n",
    "]\n",
    "\n",
    "for layer_name, _, _, _ in layers_to_upload:\n",
    "    mbtiles_path = tiles / f\"{layer_name}.mbtiles\"\n",
    "    if mbtiles_path.exists():\n",
    "        upload_to_mapbox(mbtiles_path, layer_name, st_upload_write, st_upload_read)\n",
    "    else:\n",
    "        print(f\"Skipping {layer_name} — .mbtiles not found, run Tippecanoe cell first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3bdeb0",
   "metadata": {},
   "source": [
    "## PA Harvest Lookup\n",
    "Aids us in styling and creating additional layers within the map without recreating the tiles multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aab41c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years: [2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "Years: 2009 – 2024  |  WMUs: 23\n",
      "Saved → C:\\data\\repos\\onx_hunt\\maps\\data\\pa_wmu_harvest.js\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "harvest = gpd.read_file(\"../data/misc/harvest/harvest_v1.gpkg\")\n",
    "\n",
    "print(f\"Years: {sorted(harvest['LicenseYear'].dropna().unique().astype(int).tolist())}\")\n",
    "\n",
    "# Aggregate per year × WMU\n",
    "agg = (\n",
    "    harvest\n",
    "    .groupby([\"LicenseYear\", \"WMU_ID\"])[\"Antlered_Deer_Harvested_per_Squ\"]\n",
    "    .mean()\n",
    "    .round(3)\n",
    ")\n",
    "\n",
    "# Nested dict: { \"2009\": { \"2B\": 3.21, ... }, \"2010\": { ... }, ... }\n",
    "by_year = {}\n",
    "for (year, wmu_id), val in agg.items():\n",
    "    yr = str(int(year))\n",
    "    by_year.setdefault(yr, {})[str(wmu_id)] = val\n",
    "\n",
    "years = sorted(by_year.keys())\n",
    "wmus  = sorted({wmu for data in by_year.values() for wmu in data})\n",
    "print(f\"Years: {years[0]} – {years[-1]}  |  WMUs: {len(wmus)}\")\n",
    "\n",
    "# Write as a JS file — works with file:// unlike fetch()\n",
    "out_dir = Path(\"../maps/data\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "js_path = out_dir / \"pa_wmu_harvest.js\"\n",
    "js_path.write_text(f\"const paHarvestData = {json.dumps(by_year, indent=2)};\")\n",
    "\n",
    "print(f\"Saved → {js_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
